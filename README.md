# llm-prompt-strategy-evaluation
Comparative analysis of prompt strategies on LLMs (GPT-4o, Grok 3, Claude Sonnet 4, DeepSeek V3)

# ğŸ§  LLM Prompt Strategy Evaluation

This repository contains the source files and results of my undergraduate thesis titled:

**"FarklÄ± Prompt Stratejilerinin PerformanslarÄ±nÄ±n BÃ¼yÃ¼k Dil Modellerinde KarÅŸÄ±laÅŸtÄ±rmalÄ± Analizi"**  
(*"Comparative Analysis of Prompt Strategies in Large Language Models"*)

ğŸ“ _Istanbul AydÄ±n University â€“ Department of Management Information Systems_  
ğŸ“… _June 2025_  
ğŸ‘©â€ğŸ“ _Author: Elifnur Åirin_

---

## ğŸ¯ Project Objective

The study evaluates how different prompt engineering strategies affect the performance of modern large language models (LLMs) like GPT-4o, Claude Sonnet 4, Grok 3, and DeepSeek V3.

Each strategy-model combination is tested on:
- âœ… Accuracy
- ğŸ’¬ Q&A response quality
- ğŸ§  Summarization performance
- ğŸ§‘â€ğŸ’» Code generation quality

---

## ğŸš€ Prompt Strategies Compared

- **Zero-shot prompting**
- **One-shot prompting**
- **Few-shot prompting**
- **Chain-of-thought (CoT) prompting**
- **Direct prompting**
- **Indirect prompting**

Each model was tested with all strategies on the same set of tasks using standardized prompts and a 0â€“100 scoring scale.

---

## ğŸ“Š Evaluation Criteria

Each LLM output was scored based on:

| Criterion            | Description                                       |
|----------------------|---------------------------------------------------|
| ğŸ” Accuracy           | Is the response factually correct?               |
| âœï¸ Clarity            | Is the response understandable and well-written? |
| ğŸ“š Detail Level       | Does it provide enough context and explanation?  |
| ğŸ¯ Relevance          | Does it match the question and context?          |

---

## ğŸ Key Findings

- **Chain-of-Thought** and **Indirect Prompting** strategies yielded the highest quality outputs across most tasks.
- **Few-shot** and **One-shot** strategies often suffered from limited depth due to overly simplified examples.

---

## ğŸ“ Project Structure

```bash
llm-prompt-strategy-evaluation/
â”‚â”€â”€ Prompt_Strategy_Thesis_Report.pdf        # Full report (in Turkish)
â”‚
â”œâ”€â”€ charts/
â”‚   â””â”€â”€ accuracy-metric.pdf
|   â””â”€â”€ code-generation-meteric.pdf
|   â””â”€â”€ q&a-metric.pdf
|   â””â”€â”€ summarizing-metric.pdf
â”œâ”€â”€ results/
â”‚   â””â”€â”€ accuracy_analysis.py
|   â””â”€â”€ code-generation_analysis.py
|   â””â”€â”€ q&a_analysis.py
|   â””â”€â”€ summarizing_analysis.py
|   â””â”€â”€ model_points.csv
